# @Author: Simon Dahan
# @Last Modified time: 2022-01-12 14:11:23
resolution:
  ico: 6            ## full mesh resolution
  sub_ico: 2        ## patch resolution

data:
  data_path: ./code/cv_utils/data/{}/{}
  configuration: template #template # native
  num_classes: 3
  log_data: False
  log_statistics: True

tune:
  lr:
    a: -7
    b: -2
  use_scheduler: [True, False]
  task: [alzheimer, alzheimer_bigger_train, alzheimer_oversampled_train]
  num_trials: 70
  batch_sizes: [4, 8, 16, 24, 32]
  epochs: 150
  use_gpu: True
  architecture: [xxtiny, xtiny, tiny]
  load_weights: [None, imagenet]
  finetuning: [False]
  add_noise: [True, False]
  noise_scale:
    a: -3
    b: -1
  temperature:
    a: -2
    b: 2
  dropout: 
    a: 0 # start of uniform
    b: 1 # end of uniform
  emb_dropout:
    a: 0
    b: 1
  weight_decay: # good range if decoupled: 0.025 to 0.05 
    a: 0.015
    b: 0.06
  optimisers: [SGD, Adam, AdamW]
  schedulers: [None, CosineDecay, StepLR, ReduceLROnPlateau]
  warmup: [True, False]
  early_stopping: [True, False]

setup:
  mode: training
  batch_size_val: 32
  testing: True
  val_epoch: 10
  save_ckpt: True
  monitor: metrics/BalancedAccuracy/val
  monitor_mode: max

early_stopping:
    min_delta: 0.05
    patience: 3 # only evaluated every val_epoch
    divergence_threshold: 0.33

weights:
  wandb: # uses current user and project if not explicitly specified
    xtiny: "model-ce5ca_00000:v13"
    tiny: "moritzschueler96/svit/model-ce5ca_00000:best"
    small: "moritzschueler96/svit/model-2677a_00000:latest"
    base: "moritzschueler96/svit/model-2677a_00000:best"
  imagenet:
    tiny: 'vit_tiny_patch16_224' #ViT(dim=192, depth=12, heads=3,mlp_dim=768,dim_head=64)
    small: 'vit_small_patch16_224' #ViT(dim=384, depth=12, heads=6,mlp_dim=1536,dim_head=64)
    base: 'vit_base_patch16_224' #ViT(dim=768, depth=12, heads=12,mlp_dim=3072,dim_head=64)
            
transformer:
  xxxtiny:
    dim: 48
    depth: 6
    heads: 1
    mlp_dim: 192 # 4*dim according to DeiT
    pool: 'cls'
    dim_head: 32
  xxtiny:
    dim: 48
    depth: 12
    heads: 1
    mlp_dim: 192 # 4*dim according to DeiT
    pool: 'cls'
    dim_head: 64
  xtiny:
    dim: 96
    depth: 12
    heads: 2
    mlp_dim: 384 # 4*dim according to DeiT
    pool: 'cls'
    dim_head: 64
  tiny:
    dim: 192 #192, 384, 768
    depth: 12 #12, 12, 12
    heads: 3 #3, 6, 12
    mlp_dim: 768 #768, 1536, 3072 ## 4*dim according to DeiT
    pool: 'cls'  # or 'mean'
    dim_head: 64 #64
  small:
    dim: 384 #192, 384, 768
    depth: 12 #12, 12, 12
    heads: 6 #3, 6, 12
    mlp_dim: 1536 #768, 1536, 3072 ## 4*dim according to DeiT
    pool: 'cls'  # or 'mean'
    dim_head: 64 #64
  base:
    dim: 768 #192, 384, 768
    depth: 12 #12, 12, 12
    heads: 12 #3, 6, 12
    mlp_dim: 3072 #768, 1536, 3072 ## 4*dim according to DeiT
    pool: 'cls'  # or 'mean'
    dim_head: 64 #64

optimisation:
  optimiser: SGD
  decouple_weight_decay: True
  scheduler: ReduceLROnPlateau  # CosineDecay, StepLR, ReduceLROnPlateau
  warmup: False
  nbr_step_warmup: 50

SGD:
  momentum: 0.9
  nesterov: False
  
StepLR: 
  stepsize: 1000
  decay: 0.5

CosineDecay:
  T_max: 5000
  eta_min: 0.0001

sub_ico_0:
    num_patches: 20 
    num_vertices: 2145 

sub_ico_1:
    num_patches: 80 
    num_vertices: 561 

sub_ico_2:
    num_patches: 320
    num_vertices: 153 



